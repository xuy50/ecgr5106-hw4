# ECGR-5106 Homework 4

## Student Information
**Name:** Yang Xu 
**Student ID:** 801443244  
**Homework Number:** 4  

## GitHub Repository
[https://github.com/yourusername/ecgr5106-hw4](https://github.com/yourusername/ecgr5106-hw4)

---

## Introduction

In this homework, we explore sequence-to-sequence (seq2seq) modeling for machine translation using a GRU-based encoder-decoder architecture, both **with** and **without** attention. We address the following problems:

- **Problem 1**: English \(\to\) French translation (no attention)  
- **Problem 2**: English \(\to\) French translation (with attention)  
- **Problem 3**: French \(\to\) English translation (both no attention and with attention)

We use the provided dataset `Dataset-English_to_French.txt`, which contains parallel sentences in English and French. Our goals are to:

1. Train the models on the entire dataset (no train/validation split needed given the small size).  
2. Report training loss, validation loss, and token-level validation accuracy.  
3. Generate qualitative translations to compare with the reference sentences.

Below, we detail each problem, present training curves, and provide sample translation outputs.

---

## Problem 1: GRU-based Encoder-Decoder (English \(\to\) French)

### 1.1 Implementation
- **Notebook**: `p1_GRU_etf.ipynb`
- **Model**: A simple GRU-based seq2seq architecture:
  - **Encoder**: Embedding layer + GRU, outputs a final hidden state.
  - **Decoder**: Embedding layer + GRU, predicts one word at a time until EOS token.
- **Vocabulary**: Built from **both** English and French words in the dataset. We map each word to an index.  
- **Token-Level Accuracy**: We compute how many predicted tokens match the reference tokens at each position, divided by the total number of target tokens.

### 1.2 Training Setup
- **Hidden Size**: 256  
- **Optimizer**: SGD with learning rate 0.01  
- **Loss**: NLLLoss (word-level)  
- **Epochs**: 100  

### 1.3 Results
Below is the final training vs. validation loss curve and token-level accuracy for **p1_GRU_etf**:

![Problem 1: GRU (English->French)](images/p1_GRU_etf_output.png)

From the logs:
- Final **training loss** \(\approx 0.0122\)
- Final **validation loss** \(\approx 0.0119\)
- Final **token-level validation accuracy** = 1.0000  

The model converged to near-perfect accuracy on this dataset (which is quite small, so it can be overfit easily).

### 1.4 Sample Translations
Some qualitative examples from the notebook:

``` text
Sample Translations:
Original: He checks his email
Target  : Il vérifie ses emails
Predicted: Il vérifie ses emails
--------------------------------------------------
Original: The teacher explains the lesson
Target  : Le professeur explique la leçon
Predicted: Le professeur explique la leçon
--------------------------------------------------
Original: The dog barks loudly
Target  : Le chien aboie bruyamment
Predicted: Le chien aboie bruyamment
--------------------------------------------------
Original: The baby sleeps peacefully
Target  : Le bébé dort paisiblement
Predicted: Le bébé dort paisiblement
--------------------------------------------------
Original: We plant flowers in the garden
Target  : Nous plantons des fleurs dans le jardin
Predicted: Nous plantons des fleurs dans le jardin
--------------------------------------------------
```

We observe that the model’s predictions match the target translations exactly for these examples.

---

## Problem 2: GRU-based Encoder-Decoder with Attention (English \(\to\) French)

### 2.1 Implementation
- **Notebook**: `p2_GRU_attention_etf.ipynb`
- **Model**: We extend the above GRU-based decoder to incorporate an **Attention** mechanism:
  - The encoder now stores outputs for each time step.
  - The decoder computes attention weights over these outputs at each decoding step, forming a context vector.
  - The context vector is concatenated with the decoder’s current embedding before passing it into the GRU.

### 2.2 Training Setup
- **Hidden Size**: 256  
- **Optimizer**: SGD (lr=0.01)  
- **Loss**: NLLLoss  
- **Epochs**: 100  

### 2.3 Results
The following figure shows the training/validation loss and token-level accuracy for **p2_GRU_attention_etf**:

![Problem 2: GRU + Attention (English->French)](images/p2_GRU_attention_etf_output.png)

From the logs:
- Final **training loss** \(\approx 0.0114\)
- Final **validation loss** \(\approx 0.0107\)
- Final **token-level validation accuracy** = 1.0000  

The model again converges to perfect accuracy on this dataset.

### 2.4 Sample Translations
Some translations generated by the attention-based model:

``` text
Sample Translations (Attention Model):
Original: He works hard every day
Target  : Il travaille dur tous les jours
Predicted: Il travaille dur tous les jours
--------------------------------------------------
Original: We are friends
Target  : Nous sommes amis
Predicted: Nous sommes amis
--------------------------------------------------
Original: She dances at the party
Target  : Elle danse à la fête
Predicted: Elle danse à la fête
--------------------------------------------------
Original: She locks the door
Target  : Elle ferme la porte à clé
Predicted: Elle ferme la porte à clé
--------------------------------------------------
Original: She catches the bus
Target  : Elle attrape le bus
Predicted: Elle attrape le bus
--------------------------------------------------
```

Comparing with **Problem 1**, both models achieve near-perfect results. The attention mechanism, however, generally helps the model learn alignment between source and target tokens more effectively.

---

## Problem 3: French \(\to\) English Translation

For **Problem 3**, we reverse the direction: from French to English. We repeat both the no-attention (Problem 1) and attention-based (Problem 2) approaches.

### 3.1 GRU-based Encoder-Decoder (French \(\to\) English)
- **Notebook**: `p3_GRU_fte.ipynb`
- **Implementation**: Identical architecture to Problem 1, except we swap the dataset pairs so that the model sees French as input and English as output.
- **Training Setup**: Same hyperparameters (hidden size=256, lr=0.01, epochs=100).

#### 3.1.1 Results
![Problem 3.1: GRU (French->English)](images/p3_GRU_fte_output.png)

- Final **training loss** \(\approx 0.0122\)
- Final **validation loss** \(\approx 0.0119\)
- Final **token-level validation accuracy** = 1.0000  

Sample translations:

``` text
Sample Translations (French-to-English):
Input (French): Il vérifie ses emails
Target (English): He checks his email
Predicted (English): He checks his email
--------------------------------------------------
Input (French): Le professeur explique la leçon
Target (English): The teacher explains the lesson
Predicted (English): The teacher explains the lesson
--------------------------------------------------
Input (French): Le chien aboie bruyamment
Target (English): The dog barks loudly
Predicted (English): The dog barks loudly
--------------------------------------------------
Input (French): Le bébé dort paisiblement
Target (English): The baby sleeps peacefully
Predicted (English): The baby sleeps peacefully
--------------------------------------------------
Input (French): Nous plantons des fleurs dans le jardin
Target (English): We plant flowers in the garden
Predicted (English): We plant flowers in the garden
--------------------------------------------------
```

Again, the model achieves perfect or near-perfect performance on these samples.

### 3.2 GRU-based Encoder-Decoder with Attention (French \(\to\) English)
- **Notebook**: `p3_GRU_attention_fte.ipynb`
- **Implementation**: Same attention-based approach as Problem 2, but with reversed input-output pairs.
- **Training Setup**: Hidden size=256, lr=0.01, epochs=100.

#### 3.2.1 Results
![Problem 3.2: GRU + Attention (French->English)](images/p3_GRU_attention_fte_output.png)

- Final **training loss** \(\approx 0.0114\)
- Final **validation loss** \(\approx 0.0109\)
- Final **token-level validation accuracy** = 1.0000  

Sample translations:

``` text
Sample Translations (French-to-English, Attention Model):
Input (French): Il travaille dur tous les jours
Target (English): He works hard every day
Predicted (English): He works hard every day
--------------------------------------------------
Input (French): Nous sommes amis
Target (English): We are friends
Predicted (English): We are friends
--------------------------------------------------
Input (French): Elle danse à la fête
Target (English): She dances at the party
Predicted (English): She dances at the party
--------------------------------------------------
Input (French): Elle ferme la porte à clé
Target (English): She locks the door
Predicted (English): She locks the door
--------------------------------------------------
Input (French): Elle attrape le bus
Target (English): She catches the bus
Predicted (English): She catches the bus
--------------------------------------------------
```


### 3.3 Which Direction is More Effective?
From these experiments, both **English \(\to\) French** and **French \(\to\) English** models converge to near-perfect token accuracy on this dataset. Given the small size of the corpus, the models easily overfit. In practice, with larger and more diverse data, results may differ. However, for this assignment, the performance is effectively comparable in both directions.

---

## Summary and Observations

1. **Data Size**: The dataset is quite small, which allows all four models (with/without attention, Eng\(\to\)Fr or Fr\(\to\)Eng) to reach 100% token accuracy after enough epochs.  
2. **Attention vs. No Attention**: Attention-based models can align source and target tokens more explicitly. However, on this small dataset, both approaches eventually memorize the mapping.  
3. **English\(\leftrightarrow\)French**: Both directions achieve similar results in terms of final accuracy and loss curves.  
4. **Overfitting**: The extremely high accuracy suggests the models have overfit to the small dataset. For larger, more realistic corpora, we would need proper train/validation splits and regularization.